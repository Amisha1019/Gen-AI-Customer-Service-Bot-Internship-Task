{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoTVE5Hufk2yXVx+uVNhYq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amisha1019/Gen-AI-Customer-Service-Bot-Internship-Task/blob/main/task_1_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "!pip install pdfplumber\n",
        "!pip install python-magic\n",
        "!pip install python-dotenv\n",
        "!pip install APScheduler\n",
        "!pip install tqdm\n",
        "!pip install pydantic\n",
        "!pip install PyYAML\n",
        "!pip install sqlalchemy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YGWxHA410sRX",
        "outputId": "19afa3c2-b643-4a27-c5f5-fc6790a5e475"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0\n",
            "Collecting python-magic\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Installing collected packages: python-magic\n",
            "Successfully installed python-magic-0.4.27\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Collecting APScheduler\n",
            "  Downloading APScheduler-3.11.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: tzlocal>=3.0 in /usr/local/lib/python3.12/dist-packages (from APScheduler) (5.3.1)\n",
            "Downloading APScheduler-3.11.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: APScheduler\n",
            "Successfully installed APScheduler-3.11.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.12/dist-packages (2.0.44)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (3.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"index_path\": \"./data/faiss.index\",  # persisted FAISS file\n",
        "    \"meta_db\": \"./data/meta.db\",  # sqlite metadata\n",
        "    \"embedder\": {\n",
        "        \"model_name\": \"all-MiniLM-L6-v2\",  # sentence-transformers model\n",
        "        \"chunk_size\": 500,\n",
        "        \"chunk_overlap\": 50\n",
        "    },\n",
        "    \"sources\": [\n",
        "        {\n",
        "            \"type\": \"local_folder\",\n",
        "            \"path\": \"./sources/docs\",\n",
        "            \"include_extensions\": [\".txt\", \".md\", \".csv\", \".pdf\"]\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"urls\",\n",
        "            \"urls\": [\n",
        "                \"https://example.com/customer-faq\"\n",
        "                # add other customer docs or knowledgebase pages here\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"file\",\n",
        "            \"path\": \"./sources/special_notice.txt\"\n",
        "        }\n",
        "    ],\n",
        "    \"scheduler\": {\n",
        "        \"enabled\": True,\n",
        "        \"interval_minutes\": 10  # ingest every 10 minutes\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "6jrrPgYd1kxR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import hashlib\n",
        "import mimetypes\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "def file_hash(path: str, block_size: int = 65536) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for block in iter(lambda: f.read(block_size), b\"\"):\n",
        "            h.update(block)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def text_from_pdf(path: str) -> str:\n",
        "    import pdfplumber\n",
        "    text = []\n",
        "    with pdfplumber.open(path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text.append(page_text)\n",
        "    return \"\\n\".join(text)\n",
        "\n",
        "def text_from_csv(path: str) -> str:\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(path)\n",
        "    return df.to_csv(index=False)\n",
        "\n",
        "def fetch_url_text(url: str, timeout: int = 10) -> Tuple[str, dict]:\n",
        "    headers = {\"User-Agent\": \"GenAI-KB-Updater/1.0\"}\n",
        "    resp = requests.get(url, headers=headers, timeout=timeout)\n",
        "    resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    # Remove scripts/styles and extract visible text\n",
        "    for s in soup([\"script\", \"style\", \"noscript\"]):\n",
        "        s.decompose()\n",
        "    texts = soup.stripped_strings\n",
        "    return \" \".join(texts), {\"status_code\": resp.status_code, \"headers\": dict(resp.headers)}\n",
        "\n",
        "def guess_mime_type(path: str) -> Optional[str]:\n",
        "    m, _ = mimetypes.guess_type(path)\n",
        "    return m\n"
      ],
      "metadata": {
        "id": "u_E5Mxwu15jv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "class Embedder:\n",
        "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def embed_texts(self, texts: List[str]) -> np.ndarray:\n",
        "        # returns numpy array shape (n, dim)\n",
        "        return self.model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n"
      ],
      "metadata": {
        "id": "KmTVLS7v1_7r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import sqlite3\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from utils import file_hash, text_from_pdf, text_from_csv, fetch_url_text, guess_mime_type\n",
        "# from embedder.py import Embedder\n",
        "import faiss\n",
        "DEFAULT_DIM = 384  # matches all-MiniLM-L6-v2\n",
        "\n",
        "class MetaDB:\n",
        "    def __init__(self, db_path: str):\n",
        "        os.makedirs(os.path.dirname(db_path) or \".\", exist_ok=True)\n",
        "        self.conn = sqlite3.connect(db_path, check_same_thread=False)\n",
        "        self._init_table()\n",
        "\n",
        "    def _init_table(self):\n",
        "        cur = self.conn.cursor()\n",
        "        cur.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS chunks (\n",
        "                id TEXT PRIMARY KEY,\n",
        "                source TEXT,\n",
        "                source_type TEXT,\n",
        "                content TEXT,\n",
        "                chunk_hash TEXT,\n",
        "                created_at REAL\n",
        "            )\n",
        "        \"\"\")\n",
        "        self.conn.commit()"
      ],
      "metadata": {
        "id": "YuO5um7k2QIa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def upsert_chunk(self, chunk_id: str, source: str, source_type: str, content: str, chunk_hash: str):\n",
        "        cur = self.conn.cursor()\n",
        "        cur.execute(\"\"\"\n",
        "            INSERT INTO chunks (id, source, source_type, content, chunk_hash, created_at)\n",
        "            VALUES (?, ?, ?, ?, ?, ?)\n",
        "            ON CONFLICT(id) DO UPDATE SET\n",
        "                content=excluded.content,\n",
        "                chunk_hash=excluded.chunk_hash,\n",
        "                created_at=excluded.created_at\n",
        "        \"\"\", (chunk_id, source, source_type, content, chunk_hash, time.time()))\n",
        "        self.conn.commit()\n"
      ],
      "metadata": {
        "id": "6i43fH2o2mdy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def get_chunk(self, chunk_id: str):\n",
        "        cur = self.conn.cursor()\n",
        "        cur.execute(\"SELECT * FROM chunks WHERE id = ?\", (chunk_id,))\n",
        "        return cur.fetchone()\n",
        "\n",
        "    def all_chunks(self):\n",
        "        cur = self.conn.cursor()\n",
        "        cur.execute(\"SELECT id, source, source_type, chunk_hash, created_at FROM chunks\")\n",
        "        return cur.fetchall()"
      ],
      "metadata": {
        "id": "9QyWg1H_2rsv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FaissStore:\n",
        "    def __init__(self, index_path: str, dim: int):\n",
        "        self.index_path = index_path\n",
        "        self.dim = dim\n",
        "        os.makedirs(os.path.dirname(index_path) or \".\", exist_ok=True)\n",
        "        self.id_map = {}  # mapping from sequential internal id to chunk id\n",
        "        self._load_or_create_index()"
      ],
      "metadata": {
        "id": "texUC0GS2-pZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def _load_or_create_index(self):\n",
        "        if os.path.exists(self.index_path):\n",
        "            self.index = faiss.read_index(self.index_path)\n",
        "            # load mapping file if exists\n",
        "            map_path = self.index_path + \".meta.json\"\n",
        "            if os.path.exists(map_path):\n",
        "                with open(map_path, \"r\") as f:\n",
        "                    self.id_map = json.load(f)\n",
        "            else:\n",
        "                self.id_map = {}\n",
        "        else:\n",
        "            self.index = faiss.IndexFlatIP(self.dim)  # Inner product; use normalized vectors for cosine\n",
        "            self.id_map = {}\n",
        "            self._persist()"
      ],
      "metadata": {
        "id": "YWJAyM8C3CVn"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def add_vectors(self, ids: List[str], vectors: np.ndarray):\n",
        "        # assign sequential integer ids\n",
        "        current_count = int(self.index.ntotal)\n",
        "        n = vectors.shape[0]\n",
        "        # store mapping\n",
        "        for i, cid in enumerate(ids):\n",
        "            self.id_map[str(current_count + i)] = cid\n",
        "        # ensure float32\n",
        "        vectors = vectors.astype(\"float32\")\n",
        "        # normalize for cosine similarity\n",
        "        faiss.normalize_L2(vectors)\n",
        "        self.index.add(vectors)\n",
        "        self._persist()"
      ],
      "metadata": {
        "id": "UB1ONPsa3Hca"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def search(self, vector: np.ndarray, top_k: int = 5) -> List[Tuple[str, float]]:\n",
        "        vec = vector.astype(\"float32\")\n",
        "        faiss.normalize_L2(vec)\n",
        "        D, I = self.index.search(vec, top_k)\n",
        "        results = []\n",
        "        for dist_row, idx_row in zip(D, I):\n",
        "            for dist, iid in zip(dist_row, idx_row):\n",
        "                if iid == -1:\n",
        "                    continue\n",
        "                chunk_id = self.id_map.get(str(iid))\n",
        "                results.append((chunk_id, float(dist)))\n",
        "        return results"
      ],
      "metadata": {
        "id": "ZPtt7xAZ3MPc"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def _persist(self):\n",
        "        faiss.write_index(self.index, self.index_path)\n",
        "        map_path = self.index_path + \".meta.json\"\n",
        "        with open(map_path, \"w\") as f:\n",
        "            json.dump(self.id_map, f)"
      ],
      "metadata": {
        "id": "cBOQudt43RUI"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
        "    tokens = text.split()\n",
        "    if not tokens:\n",
        "        return []\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        chunk_tokens = tokens[i:i+chunk_size]\n",
        "        chunks.append(\" \".join(chunk_tokens))\n",
        "        i += chunk_size - overlap\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "yFEehH8m3WYj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Ingestor:\n",
        "    def __init__(self, embedder: Embedder, faiss_store: FaissStore, meta_db: MetaDB, chunk_size=500, overlap=50):\n",
        "        self.embedder = embedder\n",
        "        self.faiss_store = faiss_store\n",
        "        self.meta_db = meta_db\n",
        "        self.chunk_size = chunk_size\n",
        "        self.overlap = overlap\n",
        "\n",
        "    def process_local_file(self, path: str, source_tag: str):\n",
        "        mime = guess_mime_type(path) or \"\"\n",
        "        if path.lower().endswith(\".pdf\") or \"pdf\" in mime:\n",
        "            text = text_from_pdf(path)\n",
        "        elif path.lower().endswith(\".csv\"):\n",
        "            text = text_from_csv(path)\n",
        "        else:\n",
        "            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                text = f.read()\n",
        "        return self._process_text_source(text, f\"file:{path}\", \"file\")\n",
        "\n",
        "    def process_url(self, url: str):\n",
        "        text, meta = fetch_url_text(url)\n",
        "        # you can store meta headers like Last-Modified or ETag for change detection\n",
        "        return self._process_text_source(text, f\"url:{url}\", \"url\")\n",
        "\n",
        "    def _process_text_source(self, text: str, source_identifier: str, source_type: str):\n",
        "        # split into chunks, compute chunk hashes, embed, upsert\n",
        "        chunks = chunk_text(text, self.chunk_size, self.overlap)\n",
        "        if not chunks:\n",
        "            return 0\n",
        "        embeddings = self.embedder.embed_texts(chunks)\n",
        "        # create chunk ids deterministically: hash of source + chunk index\n",
        "        ids = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunk_id = hashlib.sha256(f\"{source_identifier}:{i}\".encode()).hexdigest()\n",
        "            chunk_hash = hashlib.sha256(chunk.encode(\"utf-8\")).hexdigest()\n",
        "            ids.append((chunk_id, chunk, chunk_hash))\n",
        "        # Now check meta_db whether chunk exists and whether hash changed\n",
        "        to_add_ids = []\n",
        "        to_add_vecs = []\n",
        "        for idx, (cid, content, chash) in enumerate(ids):\n",
        "            row = self.meta_db.get_chunk(cid)\n",
        "            if row is None or row[3] != chash:  # row fields: (id, source, source_type, content, chunk_hash, created_at)\n",
        "                to_add_ids.append(cid)\n",
        "                to_add_vecs.append(embeddings[idx])\n",
        "                self.meta_db.upsert_chunk(cid, source_identifier, source_type, content, chash)\n",
        "        if to_add_ids:\n",
        "            vecs = np.vstack(to_add_vecs)\n",
        "            self.faiss_store.add_vectors(to_add_ids, vecs)\n",
        "        return len(to_add_ids)\n"
      ],
      "metadata": {
        "id": "WpP6zPeF3fNu"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This is a placeholder for special notices or urgent updates.\n",
        "You can update this file dynamically when new announcements are available.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CTb06f6k76IL",
        "outputId": "f547ce86-c406-4620-addf-a687776ff088"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThis is a placeholder for special notices or urgent updates.\\nYou can update this file dynamically when new announcements are available.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ./sources\n",
        "!echo \"This is a placeholder notice.\" > ./sources/special_notice.txt"
      ],
      "metadata": {
        "id": "1MNtQ1dV8A0J"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import faiss\n",
        "import json\n",
        "\n",
        "class FaissStore:\n",
        "    def __init__(self, index_path: str, dim: int):\n",
        "        self.index_path = index_path\n",
        "        self.dim = dim\n",
        "        os.makedirs(os.path.dirname(index_path) or \".\", exist_ok=True)\n",
        "        self.id_map = {}  # mapping from internal ID to chunk ID\n",
        "        self._load_or_create_index()  # <-- this must exist\n",
        "\n",
        "    def _load_or_create_index(self):\n",
        "        # If index exists, load it; else create a new one\n",
        "        if os.path.exists(self.index_path):\n",
        "            self.index = faiss.read_index(self.index_path)\n",
        "            # Load mapping if exists\n",
        "            map_path = self.index_path + \".meta.json\"\n",
        "            if os.path.exists(map_path):\n",
        "                with open(map_path, \"r\") as f:\n",
        "                    self.id_map = json.load(f)\n",
        "            else:\n",
        "                self.id_map = {}\n",
        "        else:\n",
        "            self.index = faiss.IndexFlatIP(self.dim)\n",
        "            self.id_map = {}\n",
        "            self._persist()\n",
        "\n",
        "    def _persist(self):\n",
        "        faiss.write_index(self.index, self.index_path)\n",
        "        map_path = self.index_path + \".meta.json\"\n",
        "        with open(map_path, \"w\") as f:\n",
        "            json.dump(self.id_map, f)\n",
        "\n",
        "    # Add other methods: add_vectors, search\n"
      ],
      "metadata": {
        "id": "FZ-1x7c19UAH"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MSOmE0C89XCX"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32af5da9"
      },
      "source": [
        "import faiss\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "class FaissStore:\n",
        "    def __init__(self, index_path: str, dim: int):\n",
        "        self.index_path = index_path\n",
        "        self.dim = dim\n",
        "        os.makedirs(os.path.dirname(index_path) or \".\", exist_ok=True)\n",
        "        self.id_map = {}  # mapping from sequential internal id to chunk id\n",
        "        self._load_or_create_index()\n",
        "\n",
        "    def _load_or_create_index(self):\n",
        "        if os.path.exists(self.index_path):\n",
        "            self.index = faiss.read_index(self.index_path)\n",
        "            # load mapping file if exists\n",
        "            map_path = self.index_path + \".meta.json\"\n",
        "            if os.path.exists(map_path):\n",
        "                with open(map_path, \"r\") as f:\n",
        "                    self.id_map = json.load(f)\n",
        "            else:\n",
        "                self.id_map = {}\n",
        "        else:\n",
        "            self.index = faiss.IndexFlatIP(self.dim)  # Inner product; use normalized vectors for cosine\n",
        "            self.id_map = {}\n",
        "            self._persist()\n",
        "\n",
        "    def add_vectors(self, ids: List[str], vectors: np.ndarray):\n",
        "        # assign sequential integer ids\n",
        "        current_count = int(self.index.ntotal)\n",
        "        n = vectors.shape[0]\n",
        "        # store mapping\n",
        "        for i, cid in enumerate(ids):\n",
        "            self.id_map[str(current_count + i)] = cid\n",
        "        # ensure float32\n",
        "        vectors = vectors.astype(\"float32\")\n",
        "        # normalize for cosine similarity\n",
        "        faiss.normalize_L2(vectors)\n",
        "        self.index.add(vectors)\n",
        "        self._persist()\n",
        "\n",
        "    def search(self, vector: np.ndarray, top_k: int = 5) -> List[Tuple[str, float]]:\n",
        "        vec = vector.astype(\"float32\")\n",
        "        faiss.normalize_L2(vec)\n",
        "        # Reshape vector for Faiss\n",
        "        if vec.ndim == 1:\n",
        "            vec = vec.reshape(1, -1)\n",
        "        D, I = self.index.search(vec, top_k)\n",
        "        results = []\n",
        "        for dist_row, idx_row in zip(D, I):\n",
        "            for dist, iid in zip(dist_row, idx_row):\n",
        "                if iid == -1:\n",
        "                    continue\n",
        "                chunk_id = self.id_map.get(str(iid))\n",
        "                results.append((chunk_id, float(dist)))\n",
        "        # If the input vector was 1D, the output should also represent a single search result\n",
        "        if len(results) > 0 and vector.ndim == 1:\n",
        "            return results\n",
        "        return results\n",
        "\n",
        "    def _persist(self):\n",
        "        faiss.write_index(self.index, self.index_path)\n",
        "        map_path = self.index_path + \".meta.json\"\n",
        "        with open(map_path, \"w\") as f:\n",
        "            json.dump(self.id_map, f)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import faiss\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "class FaissStore:\n",
        "    def __init__(self, index_path: str, dim: int):\n",
        "        \"\"\"\n",
        "        Initializes the FAISS store. Loads existing index if present, else creates a new one.\n",
        "        Args:\n",
        "            index_path: Path to store FAISS index.\n",
        "            dim: Dimension of embeddings.\n",
        "        \"\"\"\n",
        "        self.index_path = index_path\n",
        "        self.dim = dim\n",
        "        os.makedirs(os.path.dirname(index_path) or \".\", exist_ok=True)\n",
        "        self.id_map = {}  # maps internal integer IDs to chunk IDs\n",
        "        self._load_or_create_index()  # Load or create index\n",
        "\n",
        "    def _load_or_create_index(self):\n",
        "        \"\"\"\n",
        "        Loads the FAISS index from disk if exists, else creates a new IndexFlatIP.\n",
        "        \"\"\"\n",
        "        if os.path.exists(self.index_path):\n",
        "            self.index = faiss.read_index(self.index_path)\n",
        "            map_path = self.index_path + \".meta.json\"\n",
        "            if os.path.exists(map_path):\n",
        "                with open(map_path, \"r\") as f:\n",
        "                    self.id_map = json.load(f)\n",
        "            else:\n",
        "                self.id_map = {}\n",
        "        else:\n",
        "            # Use Inner Product (cosine similarity if vectors are normalized)\n",
        "            self.index = faiss.IndexFlatIP(self.dim)\n",
        "            self.id_map = {}\n",
        "            self._persist()\n",
        "\n",
        "    def add_vectors(self, ids: List[str], vectors: np.ndarray):\n",
        "        \"\"\"\n",
        "        Adds new vectors to the index and updates mapping.\n",
        "        Args:\n",
        "            ids: List of chunk IDs (strings).\n",
        "            vectors: numpy array of shape (n, dim)\n",
        "        \"\"\"\n",
        "        if len(ids) != vectors.shape[0]:\n",
        "            raise ValueError(\"Number of IDs must match number of vectors.\")\n",
        "\n",
        "        # Normalize vectors for cosine similarity\n",
        "        vectors = vectors.astype(\"float32\")\n",
        "        faiss.normalize_L2(vectors)\n",
        "\n",
        "        # Assign sequential integer IDs\n",
        "        start_idx = int(self.index.ntotal)\n",
        "        for i, cid in enumerate(ids):\n",
        "            self.id_map[str(start_idx + i)] = cid\n",
        "\n",
        "        self.index.add(vectors)\n",
        "        self._persist()\n",
        "\n",
        "    def search(self, vector: np.ndarray, top_k: int = 5) -> List[Tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        Searches for top_k nearest neighbors in the vector store.\n",
        "        Args:\n",
        "            vector: numpy array of shape (1, dim)\n",
        "            top_k: number of nearest neighbors to retrieve\n",
        "        Returns:\n",
        "            List of tuples (chunk_id, similarity_score)\n",
        "        \"\"\"\n",
        "        vec = vector.astype(\"float32\")\n",
        "        faiss.normalize_L2(vec)\n",
        "        D, I = self.index.search(vec, top_k)\n",
        "        results = []\n",
        "        for dist_row, idx_row in zip(D, I):\n",
        "            for dist, iid in zip(dist_row, idx_row):\n",
        "                if iid == -1:\n",
        "                    continue\n",
        "                chunk_id = self.id_map.get(str(iid))\n",
        "                results.append((chunk_id, float(dist)))\n",
        "        return results\n",
        "\n",
        "    def _persist(self):\n",
        "        \"\"\"\n",
        "        Persists the FAISS index and mapping to disk.\n",
        "        \"\"\"\n",
        "        faiss.write_index(self.index, self.index_path)\n",
        "        map_path = self.index_path + \".meta.json\"\n",
        "        with open(map_path, \"w\") as f:\n",
        "            json.dump(self.id_map, f)\n"
      ],
      "metadata": {
        "id": "YisDqe9Q9vQC"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from ingest import FaissStore # Removed import\n",
        "import numpy as np\n",
        "\n",
        "# Initialize FAISS store\n",
        "store = FaissStore(index_path=\"./data/customer_service_index.faiss\", dim=384)\n",
        "\n",
        "# Example: add vectors\n",
        "vectors = np.random.rand(3, 384).astype(\"float32\")\n",
        "store.add_vectors([\"chunk1\", \"chunk2\", \"chunk3\"], vectors)\n",
        "\n",
        "# Example: search\n",
        "query_vec = np.random.rand(1, 384).astype(\"float32\")\n",
        "results = store.search(query_vec, top_k=2)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDYb1gAn90eT",
        "outputId": "a5c468aa-a4fe-42c6-a4b9-9fa4ad59bf2c"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('chunk3', 0.7678486108779907), ('chunk2', 0.7536894679069519)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import time\n",
        "from apscheduler.schedulers.background import BackgroundScheduler\n",
        "# from ingest import Embedder, FaissStore, MetaDB, Ingestor # Removed import\n",
        "\n",
        "def job(ingestor, cfg):\n",
        "    try:\n",
        "        n = ingestor.ingest_sources(cfg[\"sources\"])\n",
        "        print(f\"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Scheduler: Added {n} new chunks.\")\n",
        "    except Exception as e:\n",
        "        print(\"Scheduler job failed:\", e)\n",
        "\n",
        "def start_scheduler(cfg_path=\"config.yaml\"):\n",
        "    # cfg = yaml.safe_load(open(cfg_path)) # Removed file loading\n",
        "    cfg = config # Use the config dictionary from the notebook\n",
        "    emb = Embedder(cfg[\"embedder\"][\"model_name\"])\n",
        "    dim = emb.model.get_sentence_embedding_dimension()\n",
        "    faiss_store = FaissStore(cfg[\"index_path\"], dim) # Corrected key\n",
        "    meta_db = MetaDB(cfg[\"meta_db\"]) # Corrected key\n",
        "    ing = Ingestor(emb, faiss_store, meta_db, chunk_size=cfg[\"embedder\"][\"chunk_size\"], overlap=cfg[\"embedder\"][\"chunk_overlap\"])\n",
        "\n",
        "    sched = BackgroundScheduler()\n",
        "    interval = cfg.get(\"scheduler\", {}).get(\"interval_minutes\", 10)\n",
        "    sched.add_job(job, \"interval\", minutes=interval, args=[ing, cfg])\n",
        "    sched.start()\n",
        "    print(f\"Scheduler started; polling every {interval} minutes.\")\n",
        "    try:\n",
        "        while True:\n",
        "            time.sleep(60)\n",
        "    except (KeyboardInterrupt, SystemExit):\n",
        "        sched.shutdown()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_scheduler()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cx2g8gDe-FWR",
        "outputId": "061b60cb-3e8c-4969-eb75-97575bc5d8da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scheduler started; polling every 10 minutes.\n",
            "Scheduler job failed: 'Ingestor' object has no attribute 'ingest_sources'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import numpy as np\n",
        "from ingest import Embedder, FaissStore, MetaDB\n",
        "# If your LLM is OpenAI or other, add code to call it. Here we show a structured flow.\n",
        "\n",
        "class ChatBot:\n",
        "    def __init__(self, cfg_path=\"config.yaml\"):\n",
        "        cfg = yaml.safe_load(open(cfg_path))\n",
        "        self.cfg = cfg\n",
        "        self.embedder = Embedder(cfg[\"embedder\"][\"model_name\"])\n",
        "        self.dim = self.embedder.model.get_sentence_embedding_dimension()\n",
        "        self.store = FaissStore(cfg[\"vector_store\"][\"index_path\"], self.dim)\n",
        "        self.meta_db = MetaDB(cfg[\"vector_store\"][\"meta_db\"])\n",
        "\n",
        "    def _get_chunk_content(self, chunk_id):\n",
        "        row = self.meta_db.get_chunk(chunk_id)\n",
        "        if row:\n",
        "            # table columns: id, source, source_type, content, chunk_hash, created_at\n",
        "            return row[3]\n",
        "        return None\n",
        "\n",
        "    def answer(self, query: str, top_k: int = 5):\n",
        "        qvec = self.embedder.embed_texts([query])\n",
        "        results = self.store.search(qvec, top_k=top_k)\n",
        "        contexts = []\n",
        "        for cid, score in results:\n",
        "            content = self._get_chunk_content(cid)\n",
        "            if content:\n",
        "                contexts.append((content, score))\n",
        "        # Build prompt for LLM: combine top contexts + question\n",
        "        prompt = self._build_prompt(query, contexts)\n",
        "        # Here you would call your LLM (OpenAI, local LLM, etc.) with prompt and return response.\n",
        "        # For now we'll return the prompt and contexts for demonstration.\n",
        "        return {\"prompt\": prompt, \"contexts\": contexts}\n",
        "\n",
        "    def _build_prompt(self, query, contexts):\n",
        "        pieces = [\"You are a helpful customer support assistant. Use the following knowledge snippets to answer the question.\\n\"]\n",
        "        for i, (ctx, score) in enumerate(contexts):\n",
        "            pieces.append(f\"=== snippet {i+1} (score {score:.4f}) ===\\n{ctx}\\n\")\n",
        "        pieces.append(f\"Question: {query}\\nAnswer (concise):\")\n",
        "        return \"\\n\".join(pieces)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    bot = ChatBot()\n",
        "    q = input(\"Ask: \")\n",
        "    out = bot.answer(q)\n",
        "    print(\"\\n--- Prompt sent to LLM ---\\n\")\n",
        "    print(out[\"prompt\"][:4000])  # trim long output in example\n",
        "    print(\"\\n--- Retrieved contexts (ids omitted) ---\")\n",
        "    for c, s in out[\"contexts\"]:\n",
        "        print(f\"len {len(c)} chars, score {s:.4f}\")\n"
      ],
      "metadata": {
        "id": "HcHHgg2z_2wU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}